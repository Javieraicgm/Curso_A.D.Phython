{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'dataset'\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_dataset = torchvision.datasets.MNIST(root= image_path, train= True,\n",
    "transform= transform, download= True)\n",
    "mnist_valid_dataset = Subset(mnist_dataset,\n",
    "torch.arange(1000))\n",
    "mnist_train_dataset = Subset(mnist_dataset,\n",
    "torch.arange(1000, 11000))\n",
    "#torch.arange(1000, len(mnist_dataset)))\n",
    "mnist_test_dataset = torchvision.datasets.MNIST(root= image_path, train= False,\n",
    "transform= transform, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(mnist_train_dataset, batch_size= batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(mnist_valid_dataset, batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta implementación utilizará cpu para el entrenamiento e inferencia del modelo\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Esta implementación utilizará {device} para el entrenamiento e inferencia del modelo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomRNN(\n",
       "  (rnn1): RNN(28, 128, batch_first=True)\n",
       "  (rnn2): RNN(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int,num_classes: int, sequence_length: int):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        # Hiperparámetros\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "        # Capas\n",
    "        self.rnn1 = nn.RNN(input_size, hidden_size, num_layers,batch_first=True)\n",
    "        self.rnn2 = nn.RNN(hidden_size, hidden_size, num_layers,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        def forward(self, x):\n",
    "            x = x.reshape(-1, self.sequence_length, self.input_size)\n",
    "            out, hidden = self.rnn1(x)\n",
    "            out, hidden = self.rnn2(out)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "model = CustomRNN(input_size, hidden_size, num_layers, num_classes,sequence_length)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Javiera Carrasco\\anaconda3\\envs\\ima539\\lib\\site-packages\\torchinfo\\torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 295\u001b[0m     _ \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39mx, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Javiera Carrasco\\anaconda3\\envs\\ima539\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
      "File \u001b[1;32mc:\\Users\\Javiera Carrasco\\anaconda3\\envs\\ima539\\lib\\site-packages\\torch\\nn\\modules\\module.py:363\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \n\u001b[0;32m    355\u001b[0m \u001b[39mShould be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39m    registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] is missing the required \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mforward\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m function\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [CustomRNN] is missing the required \"forward\" function",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m summary(model,input_data\u001b[39m=\u001b[39;49m torch\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, \u001b[39m255\u001b[39;49m, (batch_size, \u001b[39m1\u001b[39;49m, \u001b[39m28\u001b[39;49m, \u001b[39m28\u001b[39;49m))\u001b[39m/\u001b[39;49m \u001b[39m255\u001b[39;49m,col_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39moutput_size\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mnum_params\u001b[39;49m\u001b[39m\"\u001b[39;49m], verbose\u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,device\u001b[39m=\u001b[39;49m device)\n",
      "File \u001b[1;32mc:\\Users\\Javiera Carrasco\\anaconda3\\envs\\ima539\\lib\\site-packages\\torchinfo\\torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m validate_user_params(\n\u001b[0;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m x, correct_input_size \u001b[39m=\u001b[39m process_input(\n\u001b[0;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[0;32m    222\u001b[0m )\n\u001b[1;32m--> 223\u001b[0m summary_list \u001b[39m=\u001b[39m forward_pass(\n\u001b[0;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m formatting \u001b[39m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[0;32m    227\u001b[0m results \u001b[39m=\u001b[39m ModelStatistics(\n\u001b[0;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[0;32m    229\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Javiera Carrasco\\anaconda3\\envs\\ima539\\lib\\site-packages\\torchinfo\\torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    303\u001b[0m     executed_layers \u001b[39m=\u001b[39m [layer \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m summary_list \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mexecuted]\n\u001b[1;32m--> 304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecuted layers up to: \u001b[39m\u001b[39m{\u001b[39;00mexecuted_layers\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[39mif\u001b[39;00m hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []"
     ]
    }
   ],
   "source": [
    "summary(model,input_data= torch.randint(0, 255, (batch_size, 1, 28, 28))/ 255,col_names=[\"output_size\", \"num_params\"], verbose= 0,device= device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_hist_train\u001b[39m.\u001b[39mcpu(), loss_hist_valid\u001b[39m.\u001b[39mcpu(), accuracy_hist_train\u001b[39m.\u001b[39mcpu(), accuracy_hist_valid\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m     47\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m hist \u001b[39m=\u001b[39m train(model, num_epochs, train_dl, valid_dl, device)\n",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, num_epochs, train_dl, valid_dl, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m accuracy_hist_valid \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(num_epochs)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mwith\u001b[39;00m tqdm(train_dl, unit\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[0;32m     11\u001b[0m         model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     12\u001b[0m         \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m tepoch:\n",
      "File \u001b[1;32mc:\\Users\\Javiera Carrasco\\anaconda3\\envs\\ima539\\lib\\site-packages\\tqdm\\notebook.py:238\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m    237\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[1;32m--> 238\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[0;32m    239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Javiera Carrasco\\anaconda3\\envs\\ima539\\lib\\site-packages\\tqdm\\notebook.py:113\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[0;32m    115\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "#Librerias\n",
    "from tqdm.notebook import tqdm\n",
    "#Entrenamiento\n",
    "def train(model, num_epochs, train_dl, valid_dl, device):\n",
    "    loss_hist_train = torch.zeros(num_epochs).to(device)\n",
    "    accuracy_hist_train = torch.zeros(num_epochs).to(device)\n",
    "    loss_hist_valid = torch.zeros(num_epochs).to(device)\n",
    "    accuracy_hist_valid = torch.zeros(num_epochs).to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        with tqdm(train_dl, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            for x_batch, y_batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch} train\")\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                pred = model(x_batch)\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
    "                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "                \n",
    "                accuracy_hist_train[epoch] += is_correct.sum()\n",
    "                tepoch.set_postfix(loss= loss_hist_train[epoch].item() /len(train_dl.dataset),accuracy= 100. * accuracy_hist_train[epoch].item() / len(train_dl.dataset))\n",
    "            loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "            accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        with tqdm(valid_dl, unit=\"batch\") as vepoch:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch in vepoch:\n",
    "                    vepoch.set_description(f\"Epoch {epoch} valid\")\n",
    "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                    \n",
    "                    pred = model(x_batch)\n",
    "                    loss = loss_fn(pred, y_batch)\n",
    "                    \n",
    "                    loss_hist_valid[epoch] += loss.item() * y_batch.size(0)\n",
    "                    is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "                    \n",
    "                    accuracy_hist_valid[epoch] += is_correct.sum()\n",
    "                    vepoch.set_postfix(loss= loss_hist_valid[epoch].item() /len(valid_dl.dataset),accuracy= 100. *accuracy_hist_valid[epoch].item() / len(valid_dl.dataset))\n",
    "            loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "            accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "    return loss_hist_train.cpu(), loss_hist_valid.cpu(), accuracy_hist_train.cpu(), accuracy_hist_valid.cpu()\n",
    "\n",
    "torch.manual_seed(1)\n",
    "hist = train(model, num_epochs, train_dl, valid_dl, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
